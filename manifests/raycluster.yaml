apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: raycluster
  namespace: ray-system
spec:
  rayVersion: "2.53.0"
  enableInTreeAutoscaling: false
  headGroupSpec:
    serviceType: LoadBalancer
    rayStartParams:
      dashboard-host: "0.0.0.0"
      num-cpus: "2"  # Head node - minimal CPUs
    template:
      spec:
        containers:
        - name: ray-head
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          ports:
          - containerPort: 6379
            name: gcs-server
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            # No limits - allow head to use more if needed
          env:
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
          - name: RAY_SCHEDULER_EVENTS
            value: "0"  # Disable autoscaler event messages
  workerGroupSpecs:
  # CPU Worker Groups - one per CPU node to set exact CPU counts
  # ergos-01: 12 CPUs
  - groupName: cpu-workers-ergos-01
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "12"  # Explicitly set to match node CPU count
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-01
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling
              memory: "8Gi"
            # No limits - allow workers to use all available resources
          env:
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
  
  # ergos-03: 13 CPUs
  - groupName: cpu-workers-ergos-03
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "13"  # Explicitly set to match node CPU count
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-03
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling
              memory: "8Gi"
            # No limits - allow workers to use all available resources
          env:
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
  
  # ergos-05: 5 CPUs
  - groupName: cpu-workers-ergos-05
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "5"  # Explicitly set to match node CPU count
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-05
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling
              memory: "8Gi"
            # No limits - allow workers to use all available resources
          env:
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
  
  # GPU Worker Groups - one per GPU node to get all GPUs on each node
  # ergos-02-nv: 12 CPUs, 3 GPUs
  - groupName: gpu-workers-ergos-02-nv
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "12"  # Explicitly set to match ergos-02-nv CPU count
      num-gpus: "3"  # All 3 GPUs on ergos-02-nv
      resources: '"{\"ergos-02-nv_gpu0\": 100, \"ergos-02-nv_gpu1\": 100, \"ergos-02-nv_gpu2\": 100}"'  # Custom resources per GPU for placement control (high value allows multiple replicas)
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-02-nv
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        volumes:
        - name: dev
          hostPath:
            path: /dev
            type: Directory
        - name: nvidia-bin
          hostPath:
            path: /usr/bin
            type: Directory
        - name: nvidia-lib
          hostPath:
            path: /usr/lib/x86_64-linux-gnu
            type: Directory
        - name: vllm-install
          emptyDir: {}
        initContainers:
        - name: install-vllm
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          command: ["/bin/bash", "-c"]
          args:
          - |
            set -e  # Exit on any error
            echo "Installing ninja..."
            # Install ninja via pip (works in all environments)
            pip install ninja || {
              echo "ERROR: ninja installation failed!"
              exit 1
            }
            # Find ninja binary installed by pip and copy to shared location
            NINJA_PATH=$(python3 -c "import ninja; import os; print(os.path.join(os.path.dirname(ninja.__file__), 'data', 'bin', 'ninja'))" 2>/dev/null || which ninja || find /home/ray -name ninja -type f 2>/dev/null | head -1)
            if [ -n "$NINJA_PATH" ] && [ -f "$NINJA_PATH" ]; then
              mkdir -p /vllm-install/bin
              cp "$NINJA_PATH" /vllm-install/bin/ninja
              chmod +x /vllm-install/bin/ninja
              echo "ninja installed and copied to /vllm-install/bin from $NINJA_PATH"
            else
              # Fallback: try to find ninja in PATH after pip install
              if command -v ninja &> /dev/null; then
                mkdir -p /vllm-install/bin
                cp $(which ninja) /vllm-install/bin/ninja
                chmod +x /vllm-install/bin/ninja
                echo "ninja found in PATH and copied to /vllm-install/bin"
              else
                echo "WARNING: ninja installed but binary not found - flashinfer may fail to compile"
              fi
            fi
            echo "Installing vllm to /vllm-install..."
            # Install vllm - pip -t creates lib/python3.12/site-packages structure
            # vLLM 0.11.1+ (latest) required for guided_json support
            pip install --no-cache-dir -t /vllm-install "vllm>=0.11.1" || {
              echo "ERROR: vllm installation failed!"
              exit 1
            }
            echo "vllm installed successfully"
            echo "Verifying installation structure..."
            # Find where vllm was actually installed
            VLLM_PATH=$(find /vllm-install -type d -name "vllm" | head -1)
            if [ -z "$VLLM_PATH" ]; then
              echo "ERROR: vllm directory not found!"
              echo "Contents of /vllm-install:"
              ls -la /vllm-install/
              find /vllm-install -type f -name "__init__.py" | head -5
              exit 1
            fi
            echo "Found vllm at: $VLLM_PATH"
            # Ensure vllm is directly importable from /vllm-install
            # Create symlink if it's in site-packages but not at root
            if [[ "$VLLM_PATH" == *"site-packages"* ]] && [ ! -d "/vllm-install/vllm" ]; then
              echo "Creating symlink for direct import..."
              ln -s "$VLLM_PATH" /vllm-install/vllm
            fi
            # Verify vllm can be imported
            echo "Testing vllm import..."
            export PYTHONPATH="/vllm-install:/vllm-install/lib/python3.12/site-packages:$PYTHONPATH"
            python3 -c "import vllm; print(f'vllm imported successfully from {vllm.__file__}')" || {
              echo "ERROR: vllm cannot be imported!"
              echo "PYTHONPATH: $PYTHONPATH"
              echo "Python sys.path test:"
              python3 -c "import sys; print('\n'.join(sys.path))"
              exit 1
            }
            echo "vllm installation verified successfully"
          volumeMounts:
          - name: vllm-install
            mountPath: /vllm-install
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling - Ray will auto-detect all 12 CPUs (no limits)
              memory: "8Gi"
              nvidia.com/gpu: "3"  # All 3 GPUs on ergos-02-nv
            limits:
              memory: "40Gi"  # ergos-02-nv has 42 GB RAM - leave 2 GB for system
              nvidia.com/gpu: "3"  # Must match request
            # No CPU limit - Ray will auto-detect all available CPUs
          volumeMounts:
          - name: dev
            mountPath: /dev
          - name: nvidia-bin
            mountPath: /host/usr/bin
            readOnly: true
          - name: nvidia-lib
            mountPath: /host/usr/lib/x86_64-linux-gnu
            readOnly: true
          - name: vllm-install
            mountPath: /vllm-install
          env:
          - name: LD_LIBRARY_PATH
            value: "/host/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
          - name: PYTHONPATH
            value: "/vllm-install:/vllm-install/lib/python3.12/site-packages:/home/ray/anaconda3/lib/python3.12/site-packages"
          - name: PATH
            value: "/vllm-install/bin:/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
          - name: RAY_HEAD_ADDRESS
            value: "raycluster-head-svc:8265"
          # CUDA memory slicing for multiple replicas per GPU
          - name: PYTORCH_ALLOC_CONF
            value: "max_split_size_mb:128,expandable_segments:True"
          - name: CUDA_DEVICE_MAX_CONNECTIONS
            value: "1"
          - name: NCCL_P2P_DISABLE
            value: "1"
          - name: NCCL_IB_DISABLE
            value: "1"
          # Shared Hugging Face cache to avoid re-downloading models
          - name: HF_HOME
            value: "/vllm-install/.cache/huggingface"
          - name: TRANSFORMERS_CACHE
            value: "/vllm-install/.cache/huggingface"
  
  # ergos-04-nv: 13 CPUs, 2 GPUs
  - groupName: gpu-workers-ergos-04-nv
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "13"  # Explicitly set to match ergos-04-nv CPU count
      num-gpus: "2"  # All 2 GPUs on ergos-04-nv
      resources: '"{\"ergos-04-nv_gpu0\": 100, \"ergos-04-nv_gpu1\": 100}"'  # Custom resources per GPU for placement control (high value allows multiple replicas)
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-04-nv
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        volumes:
        - name: dev
          hostPath:
            path: /dev
            type: Directory
        - name: nvidia-bin
          hostPath:
            path: /usr/bin
            type: Directory
        - name: nvidia-lib
          hostPath:
            path: /usr/lib/x86_64-linux-gnu
            type: Directory
        - name: vllm-install
          emptyDir: {}
        initContainers:
        - name: install-vllm
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          command: ["/bin/bash", "-c"]
          args:
          - |
            set -e  # Exit on any error
            echo "Installing ninja..."
            # Install ninja via pip (works in all environments)
            pip install ninja || {
              echo "ERROR: ninja installation failed!"
              exit 1
            }
            # Find ninja binary installed by pip and copy to shared location
            NINJA_PATH=$(python3 -c "import ninja; import os; print(os.path.join(os.path.dirname(ninja.__file__), 'data', 'bin', 'ninja'))" 2>/dev/null || which ninja || find /home/ray -name ninja -type f 2>/dev/null | head -1)
            if [ -n "$NINJA_PATH" ] && [ -f "$NINJA_PATH" ]; then
              mkdir -p /vllm-install/bin
              cp "$NINJA_PATH" /vllm-install/bin/ninja
              chmod +x /vllm-install/bin/ninja
              echo "ninja installed and copied to /vllm-install/bin from $NINJA_PATH"
            else
              # Fallback: try to find ninja in PATH after pip install
              if command -v ninja &> /dev/null; then
                mkdir -p /vllm-install/bin
                cp $(which ninja) /vllm-install/bin/ninja
                chmod +x /vllm-install/bin/ninja
                echo "ninja found in PATH and copied to /vllm-install/bin"
              else
                echo "WARNING: ninja installed but binary not found - flashinfer may fail to compile"
              fi
            fi
            echo "Installing vllm to /vllm-install..."
            # Install vllm - pip -t creates lib/python3.12/site-packages structure
            # vLLM 0.11.1+ (latest) required for guided_json support
            pip install --no-cache-dir -t /vllm-install "vllm>=0.11.1" || {
              echo "ERROR: vllm installation failed!"
              exit 1
            }
            echo "vllm installed successfully"
            echo "Verifying installation structure..."
            # Find where vllm was actually installed
            VLLM_PATH=$(find /vllm-install -type d -name "vllm" | head -1)
            if [ -z "$VLLM_PATH" ]; then
              echo "ERROR: vllm directory not found!"
              echo "Contents of /vllm-install:"
              ls -la /vllm-install/
              find /vllm-install -type f -name "__init__.py" | head -5
              exit 1
            fi
            echo "Found vllm at: $VLLM_PATH"
            # Ensure vllm is directly importable from /vllm-install
            # Create symlink if it's in site-packages but not at root
            if [[ "$VLLM_PATH" == *"site-packages"* ]] && [ ! -d "/vllm-install/vllm" ]; then
              echo "Creating symlink for direct import..."
              ln -s "$VLLM_PATH" /vllm-install/vllm
            fi
            # Verify vllm can be imported
            echo "Testing vllm import..."
            export PYTHONPATH="/vllm-install:/vllm-install/lib/python3.12/site-packages:$PYTHONPATH"
            python3 -c "import vllm; print(f'vllm imported successfully from {vllm.__file__}')" || {
              echo "ERROR: vllm cannot be imported!"
              echo "PYTHONPATH: $PYTHONPATH"
              echo "Python sys.path test:"
              python3 -c "import sys; print('\n'.join(sys.path))"
              exit 1
            }
            echo "vllm installation verified successfully"
          volumeMounts:
          - name: vllm-install
            mountPath: /vllm-install
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling - Ray will auto-detect all 13 CPUs (no limits)
              memory: "8Gi"
              nvidia.com/gpu: "2"  # All 2 GPUs on ergos-04-nv
            limits:
              memory: "18Gi"  # ergos-04-nv has 20 GB RAM - leave 2 GB for system
              nvidia.com/gpu: "2"  # Must match request
            # No CPU limit - Ray will auto-detect all available CPUs
          volumeMounts:
          - name: dev
            mountPath: /dev
          - name: nvidia-bin
            mountPath: /host/usr/bin
            readOnly: true
          - name: nvidia-lib
            mountPath: /host/usr/lib/x86_64-linux-gnu
            readOnly: true
          - name: vllm-install
            mountPath: /vllm-install
          env:
          - name: LD_LIBRARY_PATH
            value: "/host/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
          - name: PYTHONPATH
            value: "/vllm-install:/vllm-install/lib/python3.12/site-packages:/home/ray/anaconda3/lib/python3.12/site-packages"
          - name: PATH
            value: "/vllm-install/bin:/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
          - name: RAY_HEAD_ADDRESS
            value: "raycluster-head-svc:8265"
          # CUDA memory slicing for multiple replicas per GPU
          - name: PYTORCH_ALLOC_CONF
            value: "max_split_size_mb:128,expandable_segments:True"
          - name: CUDA_DEVICE_MAX_CONNECTIONS
            value: "1"
          - name: NCCL_P2P_DISABLE
            value: "1"
          - name: NCCL_IB_DISABLE
            value: "1"
          # Shared Hugging Face cache to avoid re-downloading models
          - name: HF_HOME
            value: "/vllm-install/.cache/huggingface"
          - name: TRANSFORMERS_CACHE
            value: "/vllm-install/.cache/huggingface"
  
  # ergos-06-nv: 5 CPUs, 1 GPU
  - groupName: gpu-workers-ergos-06-nv
    replicas: 1
    minReplicas: 1
    maxReplicas: 1
    rayStartParams:
      num-cpus: "5"  # Explicitly set to match ergos-06-nv CPU count
      num-gpus: "1"  # All 1 GPU on ergos-06-nv
      resources: '"{\"ergos-06-nv_gpu0\": 100}"'  # Custom resource per GPU for placement control (high value allows multiple replicas)
    template:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - ergos-06-nv
        tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        volumes:
        - name: dev
          hostPath:
            path: /dev
            type: Directory
        - name: nvidia-bin
          hostPath:
            path: /usr/bin
            type: Directory
        - name: nvidia-lib
          hostPath:
            path: /usr/lib/x86_64-linux-gnu
            type: Directory
        - name: vllm-install
          emptyDir: {}
        initContainers:
        - name: install-vllm
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          command: ["/bin/bash", "-c"]
          args:
          - |
            set -e  # Exit on any error
            echo "Installing ninja..."
            # Install ninja via pip (works in all environments)
            pip install ninja || {
              echo "ERROR: ninja installation failed!"
              exit 1
            }
            # Find ninja binary installed by pip and copy to shared location
            NINJA_PATH=$(python3 -c "import ninja; import os; print(os.path.join(os.path.dirname(ninja.__file__), 'data', 'bin', 'ninja'))" 2>/dev/null || which ninja || find /home/ray -name ninja -type f 2>/dev/null | head -1)
            if [ -n "$NINJA_PATH" ] && [ -f "$NINJA_PATH" ]; then
              mkdir -p /vllm-install/bin
              cp "$NINJA_PATH" /vllm-install/bin/ninja
              chmod +x /vllm-install/bin/ninja
              echo "ninja installed and copied to /vllm-install/bin from $NINJA_PATH"
            else
              # Fallback: try to find ninja in PATH after pip install
              if command -v ninja &> /dev/null; then
                mkdir -p /vllm-install/bin
                cp $(which ninja) /vllm-install/bin/ninja
                chmod +x /vllm-install/bin/ninja
                echo "ninja found in PATH and copied to /vllm-install/bin"
              else
                echo "WARNING: ninja installed but binary not found - flashinfer may fail to compile"
              fi
            fi
            echo "Installing vllm to /vllm-install..."
            # Install vllm - pip -t creates lib/python3.12/site-packages structure
            # vLLM 0.11.1+ (latest) required for guided_json support
            pip install --no-cache-dir -t /vllm-install "vllm>=0.11.1" || {
              echo "ERROR: vllm installation failed!"
              exit 1
            }
            echo "vllm installed successfully"
            echo "Verifying installation structure..."
            # Find where vllm was actually installed
            VLLM_PATH=$(find /vllm-install -type d -name "vllm" | head -1)
            if [ -z "$VLLM_PATH" ]; then
              echo "ERROR: vllm directory not found!"
              echo "Contents of /vllm-install:"
              ls -la /vllm-install/
              find /vllm-install -type f -name "__init__.py" | head -5
              exit 1
            fi
            echo "Found vllm at: $VLLM_PATH"
            # Ensure vllm is directly importable from /vllm-install
            # Create symlink if it's in site-packages but not at root
            if [[ "$VLLM_PATH" == *"site-packages"* ]] && [ ! -d "/vllm-install/vllm" ]; then
              echo "Creating symlink for direct import..."
              ln -s "$VLLM_PATH" /vllm-install/vllm
            fi
            # Verify vllm can be imported
            echo "Testing vllm import..."
            export PYTHONPATH="/vllm-install:/vllm-install/lib/python3.12/site-packages:$PYTHONPATH"
            python3 -c "import vllm; print(f'vllm imported successfully from {vllm.__file__}')" || {
              echo "ERROR: vllm cannot be imported!"
              echo "PYTHONPATH: $PYTHONPATH"
              echo "Python sys.path test:"
              python3 -c "import sys; print('\n'.join(sys.path))"
              exit 1
            }
            echo "vllm installation verified successfully"
          volumeMounts:
          - name: vllm-install
            mountPath: /vllm-install
        containers:
        - name: ray-worker
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: "2"  # Minimal request for scheduling - Ray will auto-detect all 5 CPUs (no limits)
              memory: "8Gi"
              nvidia.com/gpu: "1"  # All 1 GPU on ergos-06-nv
            limits:
              memory: "32Gi"  # ergos-06-nv has 34 GB RAM - leave 2 GB for system
              nvidia.com/gpu: "1"  # Must match request
            # No CPU limit - Ray will auto-detect all available CPUs
          volumeMounts:
          - name: dev
            mountPath: /dev
          - name: nvidia-bin
            mountPath: /host/usr/bin
            readOnly: true
          - name: nvidia-lib
            mountPath: /host/usr/lib/x86_64-linux-gnu
            readOnly: true
          - name: vllm-install
            mountPath: /vllm-install
          env:
          - name: LD_LIBRARY_PATH
            value: "/host/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
          - name: PYTHONPATH
            value: "/vllm-install:/vllm-install/lib/python3.12/site-packages:/home/ray/anaconda3/lib/python3.12/site-packages"
          - name: PATH
            value: "/vllm-install/bin:/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          - name: RAY_DISABLE_IMPORT_WARNING
            value: "1"
          - name: RAY_HEAD_ADDRESS
            value: "raycluster-head-svc:8265"
          # CUDA memory slicing for multiple replicas per GPU
          - name: PYTORCH_ALLOC_CONF
            value: "max_split_size_mb:128,expandable_segments:True"
          - name: CUDA_DEVICE_MAX_CONNECTIONS
            value: "1"
          - name: NCCL_P2P_DISABLE
            value: "1"
          - name: NCCL_IB_DISABLE
            value: "1"
          # Shared Hugging Face cache to avoid re-downloading models
          - name: HF_HOME
            value: "/vllm-install/.cache/huggingface"
          - name: TRANSFORMERS_CACHE
            value: "/vllm-install/.cache/huggingface"
        # VRAM monitor sidecar - reports free VRAM as custom resource
        - name: vram-monitor
          image: rayproject/ray:2.53.0-py312-cu125
          imagePullPolicy: Always
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          # Share GPU devices from worker container (same pod = shared device namespace)
          # No GPU resource request needed - devices are accessible via shared namespace
          env:
          - name: RAY_HEAD_ADDRESS
            value: "raycluster-head-svc:8265"
          command: ["python3", "-c"]
          args:
          - |
            import ray
            import subprocess
            import time
            import sys
            
            def get_free_vram_gb():
                """Get actual free VRAM across all GPUs using nvidia-smi, returns GB."""
                # Try common nvidia-smi paths
                nvidia_smi_paths = ['/usr/bin/nvidia-smi', '/usr/local/bin/nvidia-smi', 'nvidia-smi']
                for nvidia_smi in nvidia_smi_paths:
                    try:
                        result = subprocess.run(
                            [nvidia_smi, '--query-gpu=memory.free', '--format=csv,noheader,nounits'],
                            capture_output=True,
                            text=True,
                            check=True,
                            timeout=2
                        )
                        free_values_mb = [int(x.strip()) for x in result.stdout.strip().split('\n') if x.strip()]
                        free_vram_mb = sum(free_values_mb)
                        return free_vram_mb / 1024.0  # Convert MB to GB
                    except (FileNotFoundError, subprocess.CalledProcessError):
                        continue  # Try next path
                # If all paths failed
                print("Error: nvidia-smi not found in any common location", file=sys.stderr)
                return None
            
            # Wait for Ray worker to be ready
            time.sleep(5)
            
            # Connect to Ray cluster via head service
            ray.init(address="ray://raycluster-head-svc:10001", ignore_reinit_error=True)
            print("VRAM Monitor started - polling GPU memory every 0.1 seconds (reports in GB)")
            
            # Simple polling loop - update every 0.1 seconds
            while True:
                try:
                    free_vram_gb = get_free_vram_gb()
                    if free_vram_gb is not None:
                        ray.util.set_resource("VRAM", free_vram_gb)
                    
                    time.sleep(0.1)
                    
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    print("Error in monitor: " + str(e), file=sys.stderr)
                    time.sleep(1)


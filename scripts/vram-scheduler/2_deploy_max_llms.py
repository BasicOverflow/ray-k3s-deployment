"""Deploy maximum replicas of a single model to fill available VRAM."""
import os
import sys
import ray

vram_scheduler_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, vram_scheduler_dir)
   


import ray_utils
import model_orchestrator
import vram_allocator

ModelOrchestrator = model_orchestrator.ModelOrchestrator
get_vram_allocator = vram_allocator.get_vram_allocator

SUPPRESS_LOGGING = False

MODEL_NAME = "Qwen/Qwen2-0.5B-Instruct"
ACTUAL_VRAM_GB = 3.89  # Total VRAM per replica (model + KV cache)

def main():
    ray_utils.init_ray(suppress_logging=SUPPRESS_LOGGING)
    
    # Calculate max replicas based on available VRAM per node
    allocator = get_vram_allocator()
    state = ray.get(allocator.get_all_gpus.remote())
    
    # Calculate replicas per GPU, then sum
    replicas_per_gpu = []
    total_free_vram = 0
    total_reserved = 0
    
    print("\nVRAM State Details (Per-GPU):")
    for gpu_key, info in state.items():
        # Skip old Ray node IDs
        if len(gpu_key) > 50 or gpu_key.startswith('c'):
            continue
        
        gpu_total = info.get("total", 0)
        gpu_free = info.get("free", 0)  # Actual free from nvidia-smi
        gpu_available = info.get("available", 0)  # Free minus pending reservations
        gpu_pending = info.get("pending", 0)
        gpu_active = info.get("active", 0)
        total_free_vram += gpu_free
        total_reserved += gpu_pending + gpu_active
        
        print(f"  {gpu_key}:")
        print(f"    Total: {gpu_total:.2f}GB, Free: {gpu_free:.2f}GB, Available: {gpu_available:.2f}GB")
        print(f"    Pending: {gpu_pending:.2f}GB, Active: {gpu_active:.2f}GB")
        
        # Use available VRAM (free - pending) for calculation
        if gpu_available >= ACTUAL_VRAM_GB:
            # Calculate how many replicas can fit based on available memory
            gpu_replicas = int(gpu_available / ACTUAL_VRAM_GB)
            replicas_per_gpu.append((gpu_key, gpu_replicas, gpu_available, gpu_total))
    
    max_replicas = sum(replicas for _, replicas, _, _ in replicas_per_gpu)
    
    print(f"\nSummary:")
    print(f"  Total free VRAM: {total_free_vram:.2f} GB")
    print(f"  Total reserved VRAM: {total_reserved:.2f} GB")
    print(f"  VRAM per replica: {ACTUAL_VRAM_GB:.2f} GB")
    print(f"  Replicas per GPU:")
    for gpu_key, gpu_replicas, gpu_available, gpu_total in replicas_per_gpu:
        total_vram_used = gpu_replicas * ACTUAL_VRAM_GB
        print(f"    {gpu_key}: {gpu_replicas} replicas - {total_vram_used:.2f}GB used from {gpu_available:.2f}GB available")
    print(f"  Can deploy {max_replicas} replicas total")
    
    # input("hold")

    if max_replicas > 0:
        # Create model config - no pre-assignments, let replicas find GPUs dynamically
        # Ray Serve doesn't guarantee placement anyway, so pre-calculating doesn't help
        MODELS = {
            "max-llm": {
                "name": MODEL_NAME,
                "vram_gb": ACTUAL_VRAM_GB,  # Pass actual VRAM directly, no multipliers
                "replicas": max_replicas
            }
        }
        
        # Deploy using ModelOrchestrator (same approach as 1_deploy_models.py)
        orchestrator = ModelOrchestrator.remote()
        ray.get(orchestrator.apply.remote(MODELS))
        
        print(f"✅ Deployed with {max_replicas} replicas")
    else:
        print("❌ Not enough VRAM to deploy any replicas")

if __name__ == "__main__":
    main()


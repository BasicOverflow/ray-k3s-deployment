apiVersion: v1
kind: ConfigMap
metadata:
  name: vram-scheduler-scripts
  namespace: ray-system
data:
  vram_allocator.py: |
    """
    Global VRAM Allocator Actor - singleton, HA-safe, persistent.

    Per-GPU tracking: Tracks VRAM state per GPU device (node_id:gpu_id format).
    """
    import ray
    from typing import Dict, Optional

    @ray.remote(num_cpus=0)
    class VRAMAllocator:
        """Global VRAM allocator - singleton, HA-safe, persistent."""
        
        def __init__(self):
            # gpu_key (node_id:gpu_id) -> {total_gb, free_gb (from nvidia-smi), pending: {replica_id: gb}, active: {replica_id: gb}}
            self.gpus: Dict[str, Dict] = {}
            # Per-GPU initialization locks - tracks which replica is currently initializing on each GPU
            # gpu_key -> replica_id (or None if no one is initializing)
            self.gpu_locks: Dict[str, Optional[str]] = {}
        
        def update_gpu(self, node_id: str, gpu_id: int, free_gb: float, total_gb: float):
            """Update VRAM state for a GPU (called by DaemonSet).
            
            free_gb from nvidia-smi is the actual free memory on the GPU.
            """
            gpu_key = f"{node_id}:gpu{gpu_id}"
            if gpu_key not in self.gpus:
                self.gpus[gpu_key] = {
                    "total": total_gb, 
                    "free": free_gb,  # Actual free from nvidia-smi
                    "pending": {},  # Reservations not yet initialized
                    "active": {}  # Successfully initialized replicas
                }
            else:
                # Update free/total from nvidia-smi, preserve reservations
                self.gpus[gpu_key]["total"] = total_gb
                self.gpus[gpu_key]["free"] = free_gb  # Ground truth from nvidia-smi
        
        def get_available_vram(self, gpu_key: str) -> float:
            """Get truly available VRAM: nvidia-smi free minus pending reservations."""
            if gpu_key not in self.gpus:
                return 0.0
            gpu = self.gpus[gpu_key]
            pending_total = sum(gpu["pending"].values())
            # Available = actual free - pending reservations
            available = gpu["free"] - pending_total
            return max(0.0, available)
        
        def reserve(self, replica_id: str, gpu_key: str, required_gb: float) -> bool:
            """Reserve VRAM for a replica. Returns True if successful.
            
            This creates a PENDING reservation that's immediately subtracted from available.
            The reservation becomes ACTIVE when the replica successfully initializes.
            """
            if gpu_key not in self.gpus:
                return False
            
            gpu = self.gpus[gpu_key]
            available = self.get_available_vram(gpu_key)
            
            if available < required_gb:
                return False
            
            # Create pending reservation (subtracts from available immediately)
            gpu["pending"][replica_id] = required_gb
            return True
        
        def mark_initialized(self, replica_id: str, gpu_key: str):
            """Mark a replica as successfully initialized.
            
            Moves reservation from pending to active.
            """
            if gpu_key not in self.gpus:
                return
            
            gpu = self.gpus[gpu_key]
            if replica_id in gpu["pending"]:
                required_gb = gpu["pending"].pop(replica_id)
                gpu["active"][replica_id] = required_gb
        
        def release(self, replica_id: str, gpu_key: str):
            """Release VRAM reservation for a replica.
            
            Removes from both pending and active.
            """
            if gpu_key not in self.gpus:
                return
            
            gpu = self.gpus[gpu_key]
            if replica_id in gpu["pending"]:
                gpu["pending"].pop(replica_id)
            if replica_id in gpu["active"]:
                gpu["active"].pop(replica_id)
        
        def get_gpu_vram(self, gpu_key: str) -> Optional[Dict]:
            """Get VRAM info for a specific GPU, including available (free - pending)."""
            if gpu_key not in self.gpus:
                return None
            
            gpu = self.gpus[gpu_key]
            available = self.get_available_vram(gpu_key)
            return {
                "total": gpu["total"],
                "free": gpu["free"],  # Actual free from nvidia-smi
                "available": available,  # Free minus pending reservations
                "pending": sum(gpu["pending"].values()),
                "active": sum(gpu["active"].values()),
                "pending_count": len(gpu["pending"]),
                "active_count": len(gpu["active"])
            }
        
        def find_gpu_with_vram(self, required_gb: float) -> Optional[str]:
            """Find a GPU with enough available VRAM (free - pending).
            
            Returns the GPU key with the most available VRAM that has at least required_gb available.
            """
            candidates = []
            for gpu_key, gpu in self.gpus.items():
                # Skip old Ray node IDs
                if len(gpu_key) > 50 or gpu_key.startswith('c'):
                    continue
                available = self.get_available_vram(gpu_key)
                if available >= required_gb:
                    candidates.append((gpu_key, available))
            
            if not candidates:
                return None
            
            # Return GPU with most available VRAM (least-loaded selection)
            candidates.sort(key=lambda x: x[1], reverse=True)
            return candidates[0][0]
        
        def get_all_gpus(self) -> Dict:
            """Get VRAM state for all GPUs."""
            result = {}
            for gpu_key, gpu in self.gpus.items():
                available = self.get_available_vram(gpu_key)
                result[gpu_key] = {
                    "total": gpu["total"],
                    "free": gpu["free"],
                    "available": available,
                    "pending": sum(gpu["pending"].values()),
                    "active": sum(gpu["active"].values())
                }
            return result
        
        def clear_all_reservations(self) -> int:
            """Clear all reservations. Returns the number cleared."""
            total_cleared = 0
            for gpu_key in self.gpus:
                gpu = self.gpus[gpu_key]
                total_cleared += len(gpu["pending"]) + len(gpu["active"])
                gpu["pending"] = {}
                gpu["active"] = {}
            return total_cleared
        
        def clear_reservations_by_prefix(self, prefix: str) -> int:
            """Clear all reservations that start with the given prefix."""
            cleared = 0
            for gpu_key in self.gpus:
                gpu = self.gpus[gpu_key]
                # Clear from pending
                to_remove = [rid for rid in gpu["pending"].keys() if rid.startswith(prefix)]
                for rid in to_remove:
                    gpu["pending"].pop(rid)
                    cleared += 1
                # Clear from active
                to_remove = [rid for rid in gpu["active"].keys() if rid.startswith(prefix)]
                for rid in to_remove:
                    gpu["active"].pop(rid)
                    cleared += 1
            return cleared
        
        def acquire_gpu_lock(self, gpu_key: str, replica_id: str) -> bool:
            """Acquire initialization lock for a GPU. Returns True if acquired."""
            if gpu_key not in self.gpu_locks:
                self.gpu_locks[gpu_key] = replica_id
                return True
            
            if self.gpu_locks[gpu_key] is not None:
                return False
            
            self.gpu_locks[gpu_key] = replica_id
            return True
        
        def release_gpu_lock(self, gpu_key: str, replica_id: str):
            """Release initialization lock for a GPU."""
            if gpu_key in self.gpu_locks and self.gpu_locks[gpu_key] == replica_id:
                self.gpu_locks[gpu_key] = None


    def get_vram_allocator():
        """Get or create the global VRAM allocator actor."""
        try:
            return ray.get_actor("vram_allocator", namespace="system")
        except ValueError:
            return VRAMAllocator.options(
                name="vram_allocator",
                namespace="system",
                lifetime="detached"
            ).remote()

  vram_monitor.py: |
    """
    VRAM Monitor Script - runs in DaemonSet to update allocator.

    This script runs on each GPU node and continuously updates
    the global VRAM allocator with current VRAM state per GPU.
    """
    import ray
    import subprocess
    import time
    import sys
    import os
    import socket

    # Add scripts directory to path for imports
    sys.path.insert(0, "/scripts/vram-scheduler")

    from vram_allocator import get_vram_allocator

    def get_gpu_count():
        """Get number of GPUs using nvidia-smi."""
        try:
            env = os.environ.copy()
            env['LD_LIBRARY_PATH'] = '/host/usr/lib/x86_64-linux-gnu:' + env.get('LD_LIBRARY_PATH', '')
            result = subprocess.run(
                ['/host/usr/bin/nvidia-smi', '--list-gpus'],
                capture_output=True, text=True, check=True, timeout=2, env=env
            )
            return len([line for line in result.stdout.strip().split('\n') if line.strip()])
        except Exception as e:
            print(f"Error getting GPU count: {e}", file=sys.stderr, flush=True)
            return 0

    def get_gpu_vram(gpu_id):
        """Get VRAM for a specific GPU using nvidia-smi (mounted from host)."""
        try:
            env = os.environ.copy()
            env['LD_LIBRARY_PATH'] = '/host/usr/lib/x86_64-linux-gnu:' + env.get('LD_LIBRARY_PATH', '')
            # Get free VRAM for specific GPU
            result_free = subprocess.run(
                ['/host/usr/bin/nvidia-smi', '--id', str(gpu_id), '--query-gpu=memory.free', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, check=True, timeout=2, env=env
            )
            # Get total VRAM for specific GPU
            result_total = subprocess.run(
                ['/host/usr/bin/nvidia-smi', '--id', str(gpu_id), '--query-gpu=memory.total', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, check=True, timeout=2, env=env
            )
            free_gb = int(result_free.stdout.strip()) / 1024.0
            total_gb = int(result_total.stdout.strip()) / 1024.0
            return free_gb, total_gb
        except Exception as e:
            print(f"Error getting VRAM for GPU {gpu_id}: {e}", file=sys.stderr, flush=True)
            return None, None

    def main():
        # Connect to Ray cluster
        ray_address = os.getenv("RAY_ADDRESS", "ray://10.0.1.53:10001")
        allocator = None
        
        print("VRAM Monitor starting...", file=sys.stderr, flush=True)
        
        # Retry connection until successful
        while True:
            try:
                if not ray.is_initialized():
                    print(f"Connecting to Ray at {ray_address}...", file=sys.stderr, flush=True)
                    ray.init(address=ray_address, ignore_reinit_error=True)
                
                # Get the global allocator
                allocator = get_vram_allocator()
                print("VRAM Monitor connected - updating allocator every 0.5 seconds", 
                      file=sys.stderr, flush=True)
                break
                
            except Exception as e:
                print(f"Failed to connect to Ray: {e}", file=sys.stderr, flush=True)
                time.sleep(5)
        
        while True:
            try:
                # Check if Ray is still connected, reconnect if needed
                if not ray.is_initialized():
                    print("Ray connection lost, reconnecting...", file=sys.stderr, flush=True)
                    ray.init(address=ray_address, ignore_reinit_error=True)
                    allocator = get_vram_allocator()
                
                # Get number of GPUs
                num_gpus = get_gpu_count()
                
                if num_gpus > 0:
                    # Use Kubernetes node name as identifier
                    k8s_node_name = os.getenv("NODE_NAME", "unknown")
                    
                    # Find the Ray worker node that corresponds to this K8s node
                    # Ray worker pods have hostnames like "raycluster-gpu-workers-ergos-06-nv-worker-xxx"
                    ray_node_id = None
                    try:
                        nodes = ray.nodes()
                        current_hostname = os.uname().nodename if hasattr(os, 'uname') else socket.gethostname()
                        
                        for node in nodes:
                            node_hostname = node.get("NodeManagerHostname", "")
                            node_id = node.get("NodeID", "")
                            # Match by hostname or node ID containing the K8s node name
                            if k8s_node_name in node_hostname or k8s_node_name in str(node_id):
                                ray_node_id = node_id
                                break
                    except Exception as e:
                        print(f"Warning: Could not find Ray node: {e}", file=sys.stderr, flush=True)
                    
                    # Update each GPU separately
                    for gpu_id in range(num_gpus):
                        free_gb, total_gb = get_gpu_vram(gpu_id)
                        
                        if free_gb is not None and total_gb is not None:
                            # Update allocator with per-GPU state
                            ray.get(allocator.update_gpu.remote(k8s_node_name, gpu_id, free_gb, total_gb))
                            
                            # Update allocator - resources are set statically at node startup
                            # The DaemonSet just tracks VRAM state for the allocator
                            print(f"K8s Node {k8s_node_name} GPU {gpu_id}: {free_gb:.2f}GB free / {total_gb:.2f}GB total", 
                                  file=sys.stderr, flush=True)
                        else:
                            print(f"Warning: Could not get VRAM for GPU {gpu_id}", file=sys.stderr, flush=True)
                else:
                    print("Warning: No GPUs found", file=sys.stderr, flush=True)
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"Error: {e}", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc(file=sys.stderr)
                # Try to reconnect on error
                try:
                    ray.shutdown()
                except:
                    pass
                time.sleep(5)

    if __name__ == "__main__":
        main()

